# P0 Bug Fixes Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Fix 4 P0-level bugs identified in code review to enable stable training.

**Architecture:** Direct bug fixes in existing files with minimal changes to surrounding code.

**Tech Stack:** Python 3.12+, PyTorch, existing DRL infrastructure

---

## Context

This plan fixes critical bugs discovered during code review of the belief-state and centralized-critic implementation:

1. **monte_carlo_sampler.py:121** - Variable name error (`opp` undefined)
2. **mappo.py:559** - Wrong optimizer used for centralized critic
3. **mappo.py:150** - Duplicate data conversion code
4. **network.py:384** - Incomplete belief integration code that raises AttributeError

---

## Task 1: Fix MonteCarloSampler Variable Name Error

**Files:**
- Modify: `src/drl/monte_carlo_sampler.py:121-125`

**Problem:** Loop uses `opp_idx` but code references undefined `opp`.

**Step 1: Locate the buggy code**

File: `src/drl/monte_carlo_sampler.py`, lines 121-125:

```python
# 更新对手手牌（更新所有3个对手）
for opp_idx in range(3):  # 处理所有对手 0, 1, 2
    opp_player = sampled_context.players[opp_idx]

    # 从采样索引中提取手牌
    opp_indices = sampled_indices[batch_idx, opp, :]  # ← BUG: opp is undefined!
```

**Step 2: Fix the variable name**

Replace line 121:
```python
for opp_idx in range(3):  # 处理所有对手 0, 1, 2
```

With:
```python
for opp in range(3):  # 处理所有对手 0, 1, 2
```

**Step 3: Update variable reference on line 122**

Replace line 122:
```python
    opp_player = sampled_context.players[opp_idx]
```

With:
```python
    opp_player = sampled_context.players[opp]
```

**Step 4: Verify the fix**

The code should now read:

```python
# 更新对手手牌（更新所有3个对手）
for opp in range(3):  # 处理所有对手 0, 1, 2
    opp_player = sampled_context.players[opp]

    # 从采样索引中提取手牌
    opp_indices = sampled_indices[batch_idx, opp, :]  # [34]
```

**Step 5: Commit**

```bash
git add src/drl/monte_carlo_sampler.py
git commit -m "fix(monte_carlo_sampler): correct variable name in opponent loop"
```

---

## Task 2: Fix MAPPO Centralized Critic Optimizer

**Files:**
- Modify: `src/drl/mappo.py:559-565`

**Problem:** `update_centralized()` uses `self.optimizer` (actor's) instead of `self.centralized_critic_optimizer`.

**Step 1: Locate the buggy code**

File: `src/drl/mappo.py`, lines 559-565:

```python
# 更新centralized critic
self.optimizer.zero_grad()  # ← WRONG: This is actor's optimizer
critic_loss.backward()
torch.nn.utils.clip_grad_norm_(
    self.centralized_critic.parameters(), self.max_grad_norm
)
self.optimizer.step()  # ← WRONG: Updates actor with critic gradients
```

**Step 2: Replace with correct optimizer**

Replace lines 559-565 with:

```python
# 更新centralized critic
self.centralized_critic_optimizer.zero_grad()
critic_loss.backward()
torch.nn.utils.clip_grad_norm_(
    self.centralized_critic.parameters(), self.max_grad_norm
)
self.centralized_critic_optimizer.step()
```

**Step 3: Verify the fix**

The code should now use `self.centralized_critic_optimizer` for both `zero_grad()` and `step()`.

**Step 4: Commit**

```bash
git add src/drl/mappo.py
git commit -m "fix(mappo): use correct optimizer for centralized critic update"
```

---

## Task 3: Remove Duplicate Data Conversion in MAPPO

**Files:**
- Modify: `src/drl/mappo.py:150-157`

**Problem:** Data conversion is duplicated at lines 120-125 and 150-157.

**Step 1: Locate the duplicate code**

File: `src/drl/mappo.py`, lines 150-157:

```python
# 转换为张量
action_masks = torch.FloatTensor(action_masks).to(self.device)
old_actions_type = torch.LongTensor(old_actions_type).to(self.device)
old_actions_param = torch.LongTensor(old_actions_param).to(self.device)
old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)
returns = torch.FloatTensor(returns).to(self.device)
advantages = torch.FloatTensor(advantages).to(self.device)
old_values = torch.FloatTensor(old_values).to(self.device)
```

**Step 2: Verify the first conversion exists**

Lines 120-125 already do the same conversion:

```python
# 转换为张量
action_masks = torch.FloatTensor(action_masks).to(self.device)
old_actions_type = torch.LongTensor(old_actions_type).to(self.device)
old_actions_param = torch.LongTensor(old_actions_param).to(self.device)
old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)
returns = torch.FloatTensor(buffer.returns).to(self.device)
```

**Step 3: Delete the duplicate code**

Delete lines 150-157 entirely.

**Step 4: Update line 252 return statement**

Since `returns` is now computed at line 135 and converted at line 125, ensure the return statement at line 252 still references `returns` correctly (it does, as it was set at line 125).

**Step 5: Commit**

```bash
git add src/drl/mappo.py
git commit -m "refactor(mappo): remove duplicate data conversion code"
```

---

## Task 4: Remove Incomplete Belief Integration Code

**Files:**
- Modify: `src/drl/network.py:384-391`

**Problem:** Incomplete belief integration code references undefined `self.use_belief` and `belief_samples`, causing AttributeError.

**Step 1: Locate the incomplete code**

File: `src/drl/network.py`, lines 384-391 (in `ObservationEncoder.forward()`):

```python
# 信念集成（可选）
if self.use_belief and belief_samples is not None:
    # 编码信念采样
    belief_features = []
    for sample in belief_samples:
        # 每个采样包含对手的观测字典
        sample_feat = self.encoder(sample)  # [batch, hidden_dim]
        belief_features.append(sample_feat)
```

**Step 2: Delete the incomplete code**

Delete lines 384-391 entirely.

The `forward()` method should end at line 383 with:

```python
# 融合所有特征
combined = torch.cat(features, dim=-1)

return self.fusion(combined)
```

**Step 3: Verify the fix**

The method should now be:

```python
def forward(self, obs: Dict[str, torch.Tensor]) -> torch.Tensor:
    # ... feature extraction code (lines 307-381) ...
    features.append(state_feat)

    # 融合所有特征
    combined = torch.cat(features, dim=-1)

    return self.fusion(combined)
```

**Step 4: Commit**

```bash
git add src/drl/network.py
git commit -m "fix(network): remove incomplete belief integration code from ObservationEncoder"
```

---

## Task 5: Verify All Fixes

**Files:**
- Test: `tests/unit/test_bug_fixes.py` (create new file)

**Step 1: Create test file for MonteCarloSampler fix**

Create: `tests/unit/test_bug_fixes.py`

```python
"""
Test P0 bug fixes
"""
import pytest
import torch
import numpy as np
from unittest.mock import Mock, MagicMock

from src.drl.monte_carlo_sampler import MonteCarloSampler


class TestMonteCarloSamplerFix:
    """Test MonteCarloSampler variable name fix"""

    def test_build_sampled_context_processes_all_opponents(self):
        """Test that all 3 opponents are processed"""
        # Create mock context with 4 players
        mock_context = Mock()
        mock_players = []
        for i in range(4):
            player = Mock()
            player.hand_tiles = [0] * 13  # 13 tiles
            mock_players.append(player)
        mock_context.players = mock_players

        # Create sampler
        sampler = MonteCarloSampler(n_samples=1)

        # Create sampled indices tensor [batch=1, 3 opponents, 34 tile types]
        sampled_indices = torch.zeros(1, 3, 34, dtype=torch.long)
        sampled_indices[0, 0, 0] = 1  # Opponent 0 has tile 0
        sampled_indices[0, 1, 1] = 1  # Opponent 1 has tile 1
        sampled_indices[0, 2, 2] = 1  # Opponent 2 has tile 2

        # Build sampled context
        result = sampler._build_sampled_context(mock_context, sampled_indices, batch_idx=0)

        # Verify all 3 opponents were processed
        # (If the bug exists, this will raise NameError: 'opp' is not defined)
        assert result is not None
        assert len(result.players) == 4


class TestMAPPOCentralizedCriticOptimizer:
    """Test MAPPO uses correct optimizer for centralized critic"""

    def test_centralized_critic_optimizer_exists(self):
        """Test that centralized_critic_optimizer is created when centralized_critic is provided"""
        from src.drl.mappo import MAPPO
        from src.drl.network import ActorCriticNetwork, CentralizedCriticNetwork

        device = "cpu"
        actor_net = ActorCriticNetwork(hidden_dim=128).to(device)
        critic_net = CentralizedCriticNetwork(hidden_dim=128).to(device)

        mappo = MAPPO(
            network=actor_net,
            centralized_critic=critic_net,
            device=device,
        )

        # Verify centralized_critic_optimizer exists
        assert mappo.centralized_critic_optimizer is not None
        assert isinstance(mappo.centralized_critic_optimizer, torch.optim.Adam)


class TestObservationEncoderNoBeliefAttribute:
    """Test ObservationEncoder doesn't reference undefined attributes"""

    def test_observation_encoder_forward_works(self):
        """Test that ObservationEncoder.forward() doesn't raise AttributeError"""
        from src.drl.network import ObservationEncoder

        encoder = ObservationEncoder(hidden_dim=128)

        # Create a minimal observation dict
        obs = self._create_minimal_observation()

        # Should not raise AttributeError about 'use_belief' or 'belief_samples'
        try:
            result = encoder(obs)
            assert result is not None
            assert result.shape == (1, 128)  # [batch=1, hidden_dim]
        except AttributeError as e:
            if "use_belief" in str(e) or "belief_samples" in str(e):
                pytest.fail(f"ObservationEncoder references undefined attribute: {e}")
            raise

    def _create_minimal_observation(self):
        """Create a minimal valid observation dict"""
        batch_size = 1
        return {
            "global_hand": torch.zeros(batch_size, 136),
            "private_hand": torch.zeros(batch_size, 34),
            "discard_pool_total": torch.zeros(batch_size, 34),
            "wall": torch.zeros(batch_size, 82),
            "melds": {
                "action_types": torch.zeros(batch_size, 16),
                "tiles": torch.zeros(batch_size, 256),
                "group_indices": torch.zeros(batch_size, 32),
            },
            "action_history": {
                "types": torch.zeros(batch_size, 80, dtype=torch.long),
                "params": torch.zeros(batch_size, 80, dtype=torch.long),
                "players": torch.zeros(batch_size, 80, dtype=torch.long),
            },
            "special_gangs": torch.zeros(batch_size, 12),
            "current_player": torch.zeros(batch_size, 1),
            "fan_counts": torch.zeros(batch_size, 4),
            "special_indicators": torch.zeros(batch_size, 2),
            "remaining_tiles": torch.ones(batch_size, 1) * 100,
            "dealer": torch.zeros(batch_size, 1),
            "current_phase": torch.zeros(batch_size, 1),
        }
```

**Step 2: Run the tests**

```bash
pytest tests/unit/test_bug_fixes.py -v
```

Expected: All tests PASS

**Step 3: Commit the tests**

```bash
git add tests/unit/test_bug_fixes.py
git commit -m "test: add regression tests for P0 bug fixes"
```

---

## Summary

After completing all tasks:

1. ✅ MonteCarloSampler correctly processes all 3 opponents
2. ✅ MAPPO uses correct optimizer for centralized critic
3. ✅ No duplicate data conversion in MAPPO
4. ✅ ObservationEncoder doesn't reference undefined attributes
5. ✅ Regression tests added

**Estimated Time:** 30-45 minutes
**Risk Level:** Low (isolated bug fixes with minimal code changes)

---

## Related Documentation

- Original review: `.sisyphus/plans/belief-state-centralized-critic-implemented/review_implementation_v2.md`
- Implementation plan: `.sisyphus/plans/belief-state-centralized-critic-improved/bk.md`
