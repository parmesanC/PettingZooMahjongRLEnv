"""
Belief State + Centralized Critic éº»å°†æ™ºèƒ½ä½“è®­ç»ƒè„šæœ¬

å®Œæ•´çš„ CTDE (Centralized Training, Decentralized Execution) è®­ç»ƒæµç¨‹ï¼š
1. Phase 1: å…¨çŸ¥è®­ç»ƒ (Omniscient) - ä½¿ç”¨ Centralized Critic + å®Œæ•´å…¨å±€çŠ¶æ€
2. Phase 2: æ¸è¿›é®è”½ (Progressive) - ä½¿ç”¨ Centralized Critic + éƒ¨åˆ†é®è”½
3. Phase 3: çœŸå®ä¿¡æ¯ (Real) - ä½¿ç”¨ Decentralized Critic + ä¿¡å¿µé‡‡æ ·

ä½¿ç”¨æ–¹æ³•ï¼š
    # å®Œæ•´è®­ç»ƒï¼ˆ2000ä¸‡å±€ï¼Œçº¦4-6å‘¨ï¼‰
    python scripts/train_belief_mahjong.py

    # å¿«é€Ÿæµ‹è¯•ï¼ˆ10ä¸‡å±€ï¼‰
    python scripts/train_belief_mahjong.py --quick-test

    # ä»æ£€æŸ¥ç‚¹æ¢å¤
    python scripts/train_belief_mahjong.py --checkpoint checkpoints/phase2_1000000.pth --phase 2

    # è‡ªå®šä¹‰é…ç½®
    python scripts/train_belief_mahjong.py --phase1-episodes 1000000 --phase2-episodes 1000000 --phase3-episodes 1000000

ä½œè€…ï¼šæ±ªå‘œå‘œ
"""

import argparse
import sys
import os
import time
import json
from pathlib import Path
from typing import Dict, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter

from example_mahjong_env import WuhanMahjongEnv
from src.drl.trainer import NFSPTrainer
from src.drl.agent import NFSPAgentPool
from src.drl.config import get_default_config, get_quick_test_config, Config
from src.drl.curriculum import CurriculumScheduler


class BeliefMahjongTrainer:
    """
    Belief State + Centralized Critic è®­ç»ƒå™¨

    å®ç°ä¸‰é˜¶æ®µ CTDE è®­ç»ƒæµç¨‹ï¼š
    - Phase 1: Omniscient (å…¨çŸ¥) - å®Œæ•´å…¨å±€çŠ¶æ€
    - Phase 2: Progressive (æ¸è¿›) - é€æ­¥é®è”½
    - Phase 3: Real (çœŸå®) - ä¿¡å¿µé‡‡æ ·
    """

    def __init__(
        self,
        config: Optional[Config] = None,
        device: str = "cuda",
        log_dir: str = "logs/belief_mahjong",
        checkpoint_dir: str = "checkpoints",
        tensorboard_dir: str = "runs/belief_mahjong",
        use_belief: bool = True,
        use_centralized_critic: bool = True,
        n_belief_samples: int = 5,
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config: é…ç½®å¯¹è±¡
            device: è®¡ç®—è®¾å¤‡
            log_dir: æ—¥å¿—ç›®å½•
            checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
            tensorboard_dir: TensorBoard æ—¥å¿—ç›®å½•
            use_belief: æ˜¯å¦ä½¿ç”¨ä¿¡å¿µç½‘ç»œ
            use_centralized_critic: æ˜¯å¦ä½¿ç”¨ Centralized Critic
            n_belief_samples: ä¿¡å¿µé‡‡æ ·æ•°é‡
        """
        self.config = config or get_default_config()
        self.device = device
        self.log_dir = log_dir
        self.checkpoint_dir = checkpoint_dir
        self.tensorboard_dir = tensorboard_dir
        self.use_belief = use_belief
        self.use_centralized_critic = use_centralized_critic
        self.n_belief_samples = n_belief_samples

        # åˆ›å»ºç›®å½•
        os.makedirs(log_dir, exist_ok=True)
        os.makedirs(checkpoint_dir, exist_ok=True)
        os.makedirs(tensorboard_dir, exist_ok=True)

        # åˆå§‹åŒ– TensorBoard
        self.writer = SummaryWriter(tensorboard_dir)

        # å½“å‰è®­ç»ƒçŠ¶æ€
        self.current_phase = 1
        self.episode_count = 0
        self.start_time = time.time()
        self.phase_start_time = time.time()
        self.phase_start_episode = 0

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_episodes": 0,
            "total_wins": [0, 0, 0, 0],
            "total_games": 0,
            "phase_stats": {
                1: {
                    "episodes": 0,
                    "wins": [0, 0, 0, 0],
                    "start_time": None,
                    "end_time": None,
                },
                2: {
                    "episodes": 0,
                    "wins": [0, 0, 0, 0],
                    "start_time": None,
                    "end_time": None,
                },
                3: {
                    "episodes": 0,
                    "wins": [0, 0, 0, 0],
                    "start_time": None,
                    "end_time": None,
                },
            },
        }

        # è®­ç»ƒé…ç½®
        self.phase_config = {
            1: {
                "episodes": 6_666_666,  # çº¦ 1/3 æ€»è®­ç»ƒé‡
                "training_phase": 1,
                "use_centralized_critic": True,
                "description": "Omniscient (å…¨çŸ¥)",
            },
            2: {
                "episodes": 6_666_667,  # çº¦ 1/3 æ€»è®­ç»ƒé‡
                "training_phase": 2,
                "use_centralized_critic": True,
                "description": "Progressive (æ¸è¿›é®è”½)",
            },
            3: {
                "episodes": 6_666_667,  # çº¦ 1/3 æ€»è®­ç»ƒé‡
                "training_phase": 3,
                "use_centralized_critic": False,
                "description": "Real (çœŸå®ä¿¡æ¯)",
            },
        }

        # åˆå§‹åŒ–åŸºç¡€è®­ç»ƒå™¨
        self.base_trainer = None
        self.agent_pool = None

        print(f"=" * 80)
        print(f"[START] Belief State + Centralized Critic è®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"=" * 80)
        print(f"è®¾å¤‡: {device}")
        print(f"ä½¿ç”¨ä¿¡å¿µç½‘ç»œ: {use_belief}")
        print(f"ä½¿ç”¨ Centralized Critic: {use_centralized_critic}")
        print(f"ä¿¡å¿µé‡‡æ ·æ•°: {n_belief_samples}")
        print(f"=" * 80)

    def _create_environment(self, phase: int) -> WuhanMahjongEnv:
        """åˆ›å»ºç¯å¢ƒ"""
        return WuhanMahjongEnv(
            render_mode=None,
            training_phase=phase,
            enable_logging=False,
        )

    def _create_agent_pool(self, phase: int) -> NFSPAgentPool:
        """åˆ›å»ºæ™ºèƒ½ä½“æ± """
        # æ›´æ–°é…ç½®ä»¥é€‚åº”å½“å‰ phase
        self.config.mahjong.training_phase = phase

        agent_pool = NFSPAgentPool(
            config=self.config,
            device=self.device,
            num_agents=4,
            share_parameters=True,
        )

        # å¦‚æœä½¿ç”¨ä¿¡å¿µç½‘ç»œï¼Œé…ç½®ç½‘ç»œ
        if self.use_belief:
            # å¯ç”¨ Actor çš„ä¿¡å¿µé›†æˆ
            if hasattr(agent_pool, "network"):
                agent_pool.network.use_belief = True
                agent_pool.network.n_belief_samples = self.n_belief_samples

        # å¦‚æœä½¿ç”¨ centralized criticï¼Œé…ç½® MAPPO
        if self.use_centralized_critic and phase in [1, 2]:
            if hasattr(agent_pool, "mappo"):
                # å¯ç”¨ centralized critic
                agent_pool.mappo.use_dual_critic = True

        return agent_pool

    def train_phase(self, phase: int, episodes: Optional[int] = None) -> Dict:
        """
        è®­ç»ƒå•ä¸ªé˜¶æ®µ

        Args:
            phase: é˜¶æ®µ (1, 2, 3)
            episodes: è®­ç»ƒå±€æ•°ï¼ˆé»˜è®¤ä½¿ç”¨ phase_config ä¸­çš„é…ç½®ï¼‰

        Returns:
            phase_stats: é˜¶æ®µç»Ÿè®¡ä¿¡æ¯
        """
        if phase not in [1, 2, 3]:
            raise ValueError(f"Phase å¿…é¡»æ˜¯ 1, 2, 3ï¼Œè€Œä¸æ˜¯ {phase}")

        phase_config = self.phase_config[phase]
        target_episodes = episodes or phase_config["episodes"]
        training_phase = phase_config["training_phase"]
        use_centralized = phase_config["use_centralized_critic"]

        print(f"\n{'=' * 80}")
        print(f"ğŸ¯ Phase {phase}: {phase_config['description']}")
        print(f"è®­ç»ƒå±€æ•°: {target_episodes:,}")
        print(f"Training Phase: {training_phase}")
        print(f"ä½¿ç”¨ Centralized Critic: {use_centralized}")
        print(f"{'=' * 80}\n")

        # è®°å½•é˜¶æ®µå¼€å§‹
        self.current_phase = phase
        self.phase_start_time = time.time()
        self.phase_start_episode = self.episode_count
        self.stats["phase_stats"][phase]["start_time"] = time.time()

        # TensorBoard è®°å½• phase è½¬æ¢äº‹ä»¶
        self.writer.add_text(
            "Phase_Transition",
            f"Phase {phase} started: {phase_config['description']}",
            global_step=self.episode_count,
        )

        # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“æ± 
        env = self._create_environment(training_phase)
        agent_pool = self._create_agent_pool(training_phase)

        # å¦‚æœæ˜¯ Phase 2 æˆ– 3ï¼Œå°è¯•ä»ä¸Šä¸€ä¸ª phase åŠ è½½æ£€æŸ¥ç‚¹
        if phase > 1:
            self._load_phase_checkpoint(phase - 1, agent_pool)

        # åˆ›å»ºè¯¾ç¨‹å­¦ä¹ è°ƒåº¦å™¨
        curriculum = CurriculumScheduler(total_episodes=target_episodes)

        # è®­ç»ƒå¾ªç¯
        episode_wins = [0, 0, 0, 0]
        eval_results = []

        for episode in range(target_episodes):
            # è¿è¡Œä¸€å±€
            episode_stats = self._run_episode(env, agent_pool, training_phase)

            # æ›´æ–°ç»Ÿè®¡
            self.episode_count += 1
            if episode_stats.get("winner") is not None:
                winner = episode_stats["winner"]
                episode_wins[winner] += 1
                self.stats["total_wins"][winner] += 1

            # å®šæœŸè¯„ä¼°
            if episode > 0 and episode % self.config.training.eval_interval == 0:
                eval_stats = self._evaluate(env, agent_pool)
                eval_results.append(eval_stats)
                self._log_eval(phase, episode, eval_stats)

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if episode > 0 and episode % self.config.training.actual_save_interval == 0:
                self._save_checkpoint(phase, episode, agent_pool)

            # å®šæœŸæ‰“å°è¿›åº¦
            if episode > 0 and episode % 1000 == 0:
                self._print_progress(phase, episode, target_episodes, episode_wins)

            # TensorBoard è®°å½•
            if episode > 0 and episode % 100 == 0:
                self._log_tensorboard(phase, episode, episode_stats)

        # é˜¶æ®µç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        self._save_checkpoint(phase, target_episodes, agent_pool, is_final=True)

        # è®°å½•é˜¶æ®µç»Ÿè®¡
        self.stats["phase_stats"][phase]["episodes"] = target_episodes
        self.stats["phase_stats"][phase]["wins"] = episode_wins
        self.stats["phase_stats"][phase]["end_time"] = time.time()

        phase_duration = time.time() - self.phase_start_time

        # TensorBoard è®°å½• phase ç»“æŸäº‹ä»¶
        phase_summary = {
            "phase": phase,
            "episodes": target_episodes,
            "wins": episode_wins,
            "duration_hours": phase_duration / 3600,
            "win_rates": [w / max(target_episodes, 1) for w in episode_wins],
        }

        self.writer.add_text(
            "Phase_Summary",
            f"Phase {phase} completed: {phase_summary}",
            global_step=self.episode_count,
        )

        # è®°å½• phase æŒ‡æ ‡
        self.writer.add_scalar(
            f"Phase{phase}/Duration_Hours", phase_duration / 3600, self.episode_count
        )
        self.writer.add_scalar(
            f"Phase{phase}/Episodes", target_episodes, self.episode_count
        )

        print(f"\n[DONE] Phase {phase} å®Œæˆï¼è€—æ—¶: {phase_duration / 3600:.2f} å°æ—¶")

        return {
            "phase": phase,
            "episodes": target_episodes,
            "wins": episode_wins,
            "duration": phase_duration,
            "eval_results": eval_results,
        }

    def _run_episode(
        self, env: WuhanMahjongEnv, agent_pool: NFSPAgentPool, training_phase: int
    ) -> Dict:
        """è¿è¡Œä¸€å±€æ¸¸æˆ"""
        obs, _ = env.reset()
        done = False
        episode_data = {"winner": None, "steps": 0, "rewards": [0.0] * 4}

        while not done:
            current_agent = env.agent_selection
            agent_id = int(current_agent.split("_")[-1])

            # è·å–è§‚æµ‹å’ŒåŠ¨ä½œæ©ç 
            agent_obs = obs[current_agent]
            action_mask = env.infos[current_agent].get("action_mask", np.ones(145))

            # é€‰æ‹©åŠ¨ä½œ
            action_type, action_param = agent_pool.select_action(
                agent_id, agent_obs, action_mask
            )

            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, rewards, terminations, truncations, infos = env.step(
                (action_type, action_param)
            )

            # å­˜å‚¨è½¬ç§»ï¼ˆå¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ï¼‰
            if agent_pool.is_training:
                reward = rewards[current_agent]
                done_flag = terminations[current_agent] or truncations[current_agent]

                agent_pool.store_transition(
                    agent_id=agent_id,
                    observation=agent_obs,
                    action=(action_type, action_param),
                    reward=reward,
                    next_observation=next_obs[current_agent],
                    done=done_flag,
                    action_mask=action_mask,
                )

            # æ›´æ–°ç»Ÿè®¡
            episode_data["steps"] += 1
            for i in range(4):
                episode_data["rewards"][i] += rewards.get(f"player_{i}", 0.0)

            # æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ
            if any(terminations.values()) or any(truncations.values()):
                done = True
                # æ‰¾å‡ºè·èƒœè€…
                for agent, term in terminations.items():
                    if term and rewards.get(agent, 0) > 0:
                        agent_id = int(agent.split("_")[-1])
                        episode_data["winner"] = agent_id
                        break

            obs = next_obs

        # è®­ç»ƒä¸€æ­¥
        train_stats = agent_pool.train_all(training_phase=training_phase)
        episode_data["train_stats"] = train_stats

        return episode_data

    def _evaluate(self, env: WuhanMahjongEnv, agent_pool: NFSPAgentPool) -> Dict:
        """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
        eval_wins = [0, 0, 0, 0]
        num_games = self.config.training.eval_games

        # ä¸´æ—¶åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        was_training = agent_pool.is_training
        agent_pool.is_training = False

        for _ in range(num_games):
            obs, _ = env.reset()
            done = False

            while not done:
                current_agent = env.agent_selection
                agent_id = int(current_agent.split("_")[-1])

                agent_obs = obs[current_agent]
                action_mask = env.infos[current_agent].get("action_mask", np.ones(145))

                action_type, action_param = agent_pool.select_action(
                    agent_id, agent_obs, action_mask
                )

                obs, rewards, terminations, truncations, infos = env.step(
                    (action_type, action_param)
                )

                if any(terminations.values()) or any(truncations.values()):
                    done = True
                    for agent, term in terminations.items():
                        if term and rewards.get(agent, 0) > 0:
                            winner_id = int(agent.split("_")[-1])
                            eval_wins[winner_id] += 1
                            break

        # æ¢å¤è®­ç»ƒæ¨¡å¼
        agent_pool.is_training = was_training

        # è®¡ç®—èƒœç‡
        total_games = sum(eval_wins)
        win_rates = [w / max(total_games, 1) for w in eval_wins]

        return {
            "wins": eval_wins,
            "win_rates": win_rates,
            "total_games": total_games,
        }

    def _log_eval(self, phase: int, episode: int, eval_stats: Dict):
        """è®°å½•è¯„ä¼°ç»“æœ"""
        print(f"[Phase {phase} - Episode {episode:,}] è¯„ä¼°ç»“æœ:")
        print(f"  æ€»å±€æ•°: {eval_stats['total_games']}")
        for i in range(4):
            print(f"  Player {i} èƒœç‡: {eval_stats['win_rates'][i] * 100:.1f}%")

        # TensorBoard è®°å½•è¯„ä¼°ç»“æœ
        self.writer.add_scalar(
            f"Phase{phase}/Eval_Total_Games",
            eval_stats["total_games"],
            self.episode_count,
        )
        for i in range(4):
            self.writer.add_scalar(
                f"Phase{phase}/Eval_Win_Rate_Player{i}",
                eval_stats["win_rates"][i],
                self.episode_count,
            )
        self.writer.add_scalar(
            f"Phase{phase}/Eval_Average_Win_Rate",
            sum(eval_stats["win_rates"]) / 4,
            self.episode_count,
        )

    def _log_tensorboard(self, phase: int, episode: int, episode_stats: Dict):
        """è®°å½• TensorBoard"""
        global_step = self.episode_count

        # è®°å½•è®­ç»ƒç»Ÿè®¡
        if "train_stats" in episode_stats:
            stats = episode_stats["train_stats"]
            if "loss" in stats:
                self.writer.add_scalar(f"Phase{phase}/Loss", stats["loss"], global_step)
            if "value_loss" in stats:
                self.writer.add_scalar(
                    f"Phase{phase}/Value_Loss", stats["value_loss"], global_step
                )
            if "policy_loss" in stats:
                self.writer.add_scalar(
                    f"Phase{phase}/Policy_Loss", stats["policy_loss"], global_step
                )
            if "entropy" in stats:
                self.writer.add_scalar(
                    f"Phase{phase}/Entropy", stats["entropy"], global_step
                )

        # è®°å½• centralized_critic æŒ‡æ ‡
        if "train_stats" in episode_stats:
            stats = episode_stats["train_stats"]
            if "centralized_critic_loss" in stats:
                self.writer.add_scalar(
                    f"Phase{phase}/Centralized_Critic_Loss",
                    stats["centralized_critic_loss"],
                    global_step,
                )

        # è®°å½• belief network æŒ‡æ ‡
        if "train_stats" in episode_stats:
            stats = episode_stats["train_stats"]
            if "belief_loss" in stats:
                self.writer.add_scalar(
                    f"Phase{phase}/Belief_Loss", stats["belief_loss"], global_step
                )
            if "belief_entropy" in stats:
                self.writer.add_scalar(
                    f"Phase{phase}/Belief_Entropy", stats["belief_entropy"], global_step
                )

        # è®°å½•æ¸¸æˆç»Ÿè®¡
        self.writer.add_scalar(
            f"Phase{phase}/Steps", episode_stats.get("steps", 0), global_step
        )

        # è®°å½•æ¸¸æˆæ—¶é•¿
        if "duration" in episode_stats:
            self.writer.add_scalar(
                f"Phase{phase}/Duration", episode_stats["duration"], global_step
            )

        # è®°å½•èƒœç‡
        total_games = sum(self.stats["total_wins"])
        if total_games > 0:
            for i in range(4):
                win_rate = self.stats["total_wins"][i] / total_games
                self.writer.add_scalar(
                    f"Phase{phase}/Win_Rate_Player{i}", win_rate, global_step
                )

        # è®°å½•å¹³å‡èƒœç‡
        avg_win_rate = sum(self.stats["total_wins"]) / (4 * max(total_games, 1))
        self.writer.add_scalar(
            f"Phase{phase}/Average_Win_Rate", avg_win_rate, global_step
        )

    def _print_progress(self, phase: int, episode: int, target: int, wins: list):
        """æ‰“å°è®­ç»ƒè¿›åº¦"""
        elapsed = time.time() - self.phase_start_time
        eps_per_sec = episode / max(elapsed, 1)
        eta = (target - episode) / max(eps_per_sec, 1)

        total_games = sum(wins)
        win_rates = [w / max(total_games, 1) * 100 for w in wins]

        print(
            f"[Phase {phase}] Episode {episode:,}/{target:,} "
            f"({episode / target * 100:.1f}%) | "
            f"Speed: {eps_per_sec:.1f} eps/s | "
            f"ETA: {eta / 3600:.1f}h | "
            f"Wins: {win_rates[0]:.1f}%/{win_rates[1]:.1f}%/{win_rates[2]:.1f}%/{win_rates[3]:.1f}%"
        )

    def _save_checkpoint(
        self,
        phase: int,
        episode: int,
        agent_pool: NFSPAgentPool,
        is_final: bool = False,
    ):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        suffix = "final" if is_final else f"{episode}"
        checkpoint_path = os.path.join(
            self.checkpoint_dir, f"phase{phase}_{suffix}.pth"
        )

        checkpoint = {
            "phase": phase,
            "episode": episode,
            "global_episode": self.episode_count,
            # æ‰‹åŠ¨ä¿å­˜ç½‘ç»œçŠ¶æ€
            "best_response_net_state": agent_pool.shared_nfsp.best_response_net.state_dict(),
            "average_policy_net_state": agent_pool.shared_nfsp.average_policy_net.state_dict(),
            "centralized_critic_state": agent_pool.shared_nfsp.centralized_critic.state_dict()
            if agent_pool.shared_nfsp.centralized_critic is not None
            else None,
            "stats": self.stats,
            "config": self.config,
            "timestamp": time.time(),
        }

        torch.save(checkpoint, checkpoint_path)
        print(f"[SAVE] æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

        # åŒæ—¶ä¿å­˜ä¸ºæœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(self.checkpoint_dir, "latest.pth")
        torch.save(checkpoint, latest_path)

    def _load_phase_checkpoint(self, from_phase: int, agent_pool: NFSPAgentPool):
        """ä»ä¸Šä¸€ä¸ª phase åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint_path = os.path.join(
            self.checkpoint_dir, f"phase{from_phase}_final.pth"
        )

        if not os.path.exists(checkpoint_path):
            print(f"[WARN]  Phase {from_phase} çš„æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            print(f"   å°†ä»å¤´å¼€å§‹è®­ç»ƒ Phase {self.current_phase}")
            return False

        try:
            # ä½¿ç”¨ weights_only=False åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆPyTorch 2.6+ çš„é»˜è®¤å€¼ï¼‰
            checkpoint = torch.load(
                checkpoint_path, map_location=self.device, weights_only=False
            )

            # åŠ è½½ç½‘ç»œçŠ¶æ€
            if checkpoint.get("best_response_net_state") and checkpoint.get(
                "average_policy_net_state"
            ):
                agent_pool.shared_nfsp.best_response_net.load_state_dict(
                    checkpoint["best_response_net_state"]
                )
                agent_pool.shared_nfsp.average_policy_net.load_state_dict(
                    checkpoint["average_policy_net_state"]
                )

                if (
                    checkpoint.get("centralized_critic_state")
                    and agent_pool.shared_nfsp.centralized_critic is not None
                ):
                    agent_pool.shared_nfsp.centralized_critic.load_state_dict(
                        checkpoint["centralized_critic_state"]
                    )

                print(f"[DONE] å·²ä» Phase {from_phase} åŠ è½½æ£€æŸ¥ç‚¹")

                # Phase 2->3 è¿ç§»ï¼šå¦‚æœæ˜¯ä» centralized åˆ° decentralized
                if from_phase == 2 and self.current_phase == 3:
                    print(f"[TRANSITION] æ‰§è¡Œ Phase 2->3 è¿ç§»: Critic é‡æ–°åˆå§‹åŒ–")

                    # é‡æ–°åˆå§‹åŒ– Local Criticï¼ˆPhase 3 ä¸ä½¿ç”¨ Centralized Criticï¼‰
                    # æˆ‘ä»¬éœ€è¦è®¿é—® shared_nfsp ä¸­çš„ best_response_net
                    if agent_pool.share_parameters and hasattr(
                        agent_pool, "shared_nfsp"
                    ):
                        nfsp = agent_pool.shared_nfsp

                        # 1. ä¿å­˜ Actor æƒé‡ï¼ˆç­–ç•¥ç½‘ç»œï¼‰
                        actor_state_dict = {}
                        for key, value in nfsp.best_response_net.state_dict().items():
                            if key.startswith("actor_type.") or key.startswith(
                                "actor_param."
                            ):
                                actor_state_dict[key] = value.clone()

                        # 2. é‡æ–°åˆå§‹åŒ– Critic æƒé‡ï¼ˆå› ä¸ºè®­ç»ƒæ–¹å¼æ”¹å˜ï¼‰
                        # Critic ä» Phase 2 çš„ centralized è®­ç»ƒåˆ‡æ¢åˆ° Phase 3 çš„ decentralized è®­ç»ƒ
                        critic_state_dict = {}
                        for key, value in nfsp.best_response_net.state_dict().items():
                            if key.startswith("critic."):
                                # é‡æ–°åˆå§‹åŒ– critic æƒé‡
                                if hasattr(value, "data"):
                                    new_data = torch.randn_like(value.data) * 0.01
                                    # åªå¯¹å…·æœ‰è¶³å¤Ÿç»´åº¦çš„å¼ é‡åº”ç”¨ Xavier åˆå§‹åŒ–
                                    if len(new_data.shape) >= 2:
                                        nn.init.xavier_uniform_(new_data)
                                    critic_state_dict[key] = new_data

                        # 3. åº”ç”¨æ›´æ–°åçš„æƒé‡
                        with torch.no_grad():
                            for key, value in actor_state_dict.items():
                                nfsp.best_response_net.state_dict()[key].copy_(value)
                            for key, value in critic_state_dict.items():
                                nfsp.best_response_net.state_dict()[key].copy_(value)

                        print(
                            f"[TRANSITION] Actor æƒé‡å·²ä¿ç•™ï¼ŒLocal Critic å·²é‡æ–°åˆå§‹åŒ–"
                        )

                        # 4. é‡ç½® Centralized Criticï¼ˆPhase 3 ä¸å†ä½¿ç”¨ï¼‰
                        if hasattr(nfsp, "centralized_critic"):
                            nfsp.centralized_critic = None
                            print(
                                f"[TRANSITION] Centralized Critic å·²ç§»é™¤ï¼ˆPhase 3 ä¸éœ€è¦ï¼‰"
                            )

                return True
            else:
                print(f"[WARN]  æ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰ agent_pool_state")
                return False

        except Exception as e:
            print(f"[ERROR] åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False

    def train(
        self,
        phase1_episodes: Optional[int] = None,
        phase2_episodes: Optional[int] = None,
        phase3_episodes: Optional[int] = None,
    ):
        """
        æ‰§è¡Œå®Œæ•´çš„ä¸‰é˜¶æ®µè®­ç»ƒ

        Args:
            phase1_episodes: Phase 1 è®­ç»ƒå±€æ•°
            phase2_episodes: Phase 2 è®­ç»ƒå±€æ•°
            phase3_episodes: Phase 3 è®­ç»ƒå±€æ•°
        """
        print(f"\n{'=' * 80}")
        print(f"[LAUNCH] å¼€å§‹å®Œæ•´çš„ä¸‰é˜¶æ®µ CTDE è®­ç»ƒ")
        print(f"{'=' * 80}\n")

        # Phase 1: Omniscient
        if phase1_episodes is not None:
            self.phase_config[1]["episodes"] = phase1_episodes
        self.train_phase(1)

        # Phase 2: Progressive
        if phase2_episodes is not None:
            self.phase_config[2]["episodes"] = phase2_episodes
        self.train_phase(2)

        # Phase 3: Real
        if phase3_episodes is not None:
            self.phase_config[3]["episodes"] = phase3_episodes
        self.train_phase(3)

        # è®­ç»ƒå®Œæˆ
        total_duration = time.time() - self.start_time
        print(f"\n{'=' * 80}")
        print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_duration / 3600:.2f} å°æ—¶")
        print(f"{'=' * 80}\n")

        # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
        self._save_final_stats()

        # å…³é—­ TensorBoard
        self.writer.close()

    def _save_final_stats(self):
        """ä¿å­˜æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        stats_path = os.path.join(self.log_dir, "final_stats.json")

        final_stats = {
            "total_episodes": self.episode_count,
            "total_duration_hours": (time.time() - self.start_time) / 3600,
            "phase_stats": self.stats["phase_stats"],
            "total_wins": self.stats["total_wins"],
        }

        with open(stats_path, "w", encoding="utf-8") as f:
            json.dump(final_stats, f, indent=2, ensure_ascii=False)

        print(f"[STATS] æœ€ç»ˆç»Ÿè®¡å·²ä¿å­˜: {stats_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Belief State + Centralized Critic éº»å°†æ™ºèƒ½ä½“è®­ç»ƒ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å®Œæ•´è®­ç»ƒï¼ˆé»˜è®¤é…ç½®ï¼‰
  python scripts/train_belief_mahjong.py

  # å¿«é€Ÿæµ‹è¯•ï¼ˆå°é‡æ•°æ®ï¼‰
  python scripts/train_belief_mahjong.py --quick-test

  # ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆPhase 2ï¼‰
  python scripts/train_belief_mahjong.py --checkpoint checkpoints/phase1_final.pth --start-phase 2

  # è‡ªå®šä¹‰å„é˜¶æ®µå±€æ•°
  python scripts/train_belief_mahjong.py --phase1-episodes 1000000 --phase2-episodes 1000000 --phase3-episodes 1000000

  # ä¸ä½¿ç”¨ä¿¡å¿µç½‘ç»œ
  python scripts/train_belief_mahjong.py --no-belief

  # ä¸ä½¿ç”¨ Centralized Critic
  python scripts/train_belief_mahjong.py --no-centralized-critic
        """,
    )

    # è®­ç»ƒæ¨¡å¼
    parser.add_argument(
        "--quick-test",
        action="store_true",
        help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆå„é˜¶æ®µ1ä¸‡å±€ï¼‰",
    )

    parser.add_argument(
        "--start-phase",
        type=int,
        default=1,
        choices=[1, 2, 3],
        help="ä»å“ªä¸ªé˜¶æ®µå¼€å§‹è®­ç»ƒï¼ˆé»˜è®¤: 1ï¼‰",
    )

    # å„é˜¶æ®µå±€æ•°
    parser.add_argument(
        "--phase1-episodes",
        type=int,
        default=None,
        help="Phase 1 è®­ç»ƒå±€æ•°ï¼ˆé»˜è®¤: 6,666,666ï¼‰",
    )

    parser.add_argument(
        "--phase2-episodes",
        type=int,
        default=None,
        help="Phase 2 è®­ç»ƒå±€æ•°ï¼ˆé»˜è®¤: 6,666,667ï¼‰",
    )

    parser.add_argument(
        "--phase3-episodes",
        type=int,
        default=None,
        help="Phase 3 è®­ç»ƒå±€æ•°ï¼ˆé»˜è®¤: 6,666,667ï¼‰",
    )

    # æ£€æŸ¥ç‚¹
    parser.add_argument(
        "--checkpoint",
        type=str,
        default=None,
        help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ",
    )

    # è®¾å¤‡
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        choices=["cuda", "cpu"],
        help="è®¡ç®—è®¾å¤‡ï¼ˆé»˜è®¤: cuda å¦‚æœå¯ç”¨ï¼‰",
    )

    # æ¶æ„é€‰é¡¹
    parser.add_argument(
        "--no-belief",
        action="store_true",
        help="ä¸ä½¿ç”¨ä¿¡å¿µç½‘ç»œ",
    )

    parser.add_argument(
        "--no-centralized-critic",
        action="store_true",
        help="ä¸ä½¿ç”¨ Centralized Critic",
    )

    parser.add_argument(
        "--belief-samples",
        type=int,
        default=5,
        help="ä¿¡å¿µé‡‡æ ·æ•°é‡ï¼ˆé»˜è®¤: 5ï¼‰",
    )

    # ç›®å½•é…ç½®
    parser.add_argument(
        "--log-dir",
        type=str,
        default="logs/belief_mahjong",
        help="æ—¥å¿—ç›®å½•ï¼ˆé»˜è®¤: logs/belief_mahjongï¼‰",
    )

    parser.add_argument(
        "--checkpoint-dir",
        type=str,
        default="checkpoints",
        help="æ£€æŸ¥ç‚¹ç›®å½•ï¼ˆé»˜è®¤: checkpointsï¼‰",
    )

    parser.add_argument(
        "--tensorboard-dir",
        type=str,
        default="runs/belief_mahjong",
        help="TensorBoard æ—¥å¿—ç›®å½•ï¼ˆé»˜è®¤: runs/belief_mahjongï¼‰",
    )

    # éšæœºç§å­
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­ï¼ˆé»˜è®¤: 42ï¼‰",
    )

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(args.seed)

    # è·å–é…ç½®
    if args.quick_test:
        config = get_quick_test_config()
        # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šå„é˜¶æ®µ1ä¸‡å±€
        phase1_episodes = 10_000
        phase2_episodes = 10_000
        phase3_episodes = 10_000
        print("=" * 80)
        print("[LAUNCH] å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
        print("=" * 80)
    else:
        config = get_default_config()
        phase1_episodes = args.phase1_episodes
        phase2_episodes = args.phase2_episodes
        phase3_episodes = args.phase3_episodes
        print("=" * 80)
        print("[GAME] Belief State + Centralized Critic éº»å°†æ™ºèƒ½ä½“è®­ç»ƒ")
        print("=" * 80)

    # æ‰“å°é…ç½®
    print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  å¼€å§‹é˜¶æ®µ: {args.start_phase}")
    print(f"  Phase 1 å±€æ•°: {phase1_episodes or 6_666_666:,}")
    print(f"  Phase 2 å±€æ•°: {phase2_episodes or 6_666_667:,}")
    print(f"  Phase 3 å±€æ•°: {phase3_episodes or 6_666_667:,}")
    print(f"  ä½¿ç”¨ä¿¡å¿µç½‘ç»œ: {not args.no_belief}")
    print(f"  ä½¿ç”¨ Centralized Critic: {not args.no_centralized_critic}")
    print(f"  ä¿¡å¿µé‡‡æ ·æ•°: {args.belief_samples}")
    print(f"  è®¾å¤‡: {args.device}")
    print(f"  éšæœºç§å­: {args.seed}")
    print(f"  æ—¥å¿—ç›®å½•: {args.log_dir}")
    print(f"  æ£€æŸ¥ç‚¹ç›®å½•: {args.checkpoint_dir}")
    print(f"  TensorBoard: {args.tensorboard_dir}")
    if args.checkpoint:
        print(f"  æ¢å¤æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print("=" * 80)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = BeliefMahjongTrainer(
        config=config,
        device=args.device,
        log_dir=args.log_dir,
        checkpoint_dir=args.checkpoint_dir,
        tensorboard_dir=args.tensorboard_dir,
        use_belief=not args.no_belief,
        use_centralized_critic=not args.no_centralized_critic,
        n_belief_samples=args.belief_samples,
    )

    # ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.checkpoint and args.start_phase > 1:
        print(f"\nğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: {args.checkpoint}")
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ£€æŸ¥ç‚¹æ¢å¤é€»è¾‘

    # å¼€å§‹è®­ç»ƒ
    try:
        if args.start_phase == 1:
            trainer.train(phase1_episodes, phase2_episodes, phase3_episodes)
        elif args.start_phase == 2:
            trainer.train_phase(2, phase2_episodes)
            trainer.train_phase(3, phase3_episodes)
        elif args.start_phase == 3:
            trainer.train_phase(3, phase3_episodes)

        print("\n[DONE] è®­ç»ƒå®Œæˆï¼")
        print(f"æ—¥å¿—ä¿å­˜äº: {args.log_dir}")
        print(f"æ¨¡å‹ä¿å­˜äº: {args.checkpoint_dir}")
        print(f"TensorBoard: tensorboard --logdir={args.tensorboard_dir}")

    except KeyboardInterrupt:
        print("\n[WARN]  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        # ä¿å­˜ä¸­æ–­æ—¶çš„æ£€æŸ¥ç‚¹
        print("[SAVE] ä¿å­˜ä¸­æ–­æ£€æŸ¥ç‚¹...")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
